{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800174f8",
   "metadata": {},
   "source": [
    "### RAG Pipeline - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3722fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bacd086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "Processing: 4 Types of LLM Architectueres.pdf\n",
      "  ‚úì Loaded 13 pages\n",
      "\n",
      "Processing: AI AudioBook Generator.pdf\n",
      "  ‚úì Loaded 3 pages\n",
      "\n",
      "Processing: CollegePeCharcha_VITPune.pdf\n",
      "  ‚úì Loaded 10 pages\n",
      "\n",
      "Total documents loaded: 26\n"
     ]
    }
   ],
   "source": [
    "### Read all the PDFs inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ‚úì Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d102f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='T y p e s  o f  L L M\\nA r c h i t e c t u r e s\\nBhavishya Pandit'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nIntroduction\\nTransformers are everywhere. They‚Äôre the groundwork for\\nnearly every major language model today. \\nTransformers\\nMixture of Experts (MoE)\\nMamba\\nMixture of Recursion (MoR)\\nIn this post, we‚Äôll walk through the evolution of four key\\narchitectures that shaped modern AI models:\\nEach step builds on the last, and together they show where the\\nfuture of model design is heading. Let‚Äôs begin with the\\nfoundation of it all, the Transformer.\\nTransformers2017\\nMoE2021\\nMamba2023\\nMoR2025'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nSource\\nInstead of passing information step by step like RNNs or LSTMs,\\ntransfomers introduced self-attention and feed-forward layers so\\ntokens could look at each other all at once. That introduced te\\nconcept of parallelism.\\nTraining on massive datasets without crawling speeds\\nScaling models from millions to billions of parameters\\n1. Transformers\\nThink of Transformers as the moment deep learning hit turbo\\nmode.\\nThis shift unlocked two big things:'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Parallel training means they don‚Äôt have to process words\\none by one like RNNs did. Huge speed boost.\\nThey handle long-range context really well‚Äîperfect for\\nunderstanding long documents or conversations.\\nMost importantly, they scale beautifully. \\nBhavishya Pandit\\nSelf-attention computes relationships between all tokens\\nin a sequence, allowing each token to weigh the\\nimportance of others.\\nFeed-forward layers transform token representations into\\nhigher-level features.\\nStacked layers progressively refine these representations,\\nenabling deep contextual understanding.\\nEvery token, whether it‚Äôs a rare scientific term or a\\ncommon word,goes through the same heavy computation.\\nThis makes inference slow and costly, since memory and\\ncompute pile up.\\n1. Transformers\\nHow do they work:\\nWhy they became so powerful:\\nBut here‚Äôs the catch:\\nContd.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nThey tied weights across depth and applied the same block\\nrepeatedly to refine token states, like an iterative loop. It\\nintroduced the idea of per-position iterative processing.\\nIt also explored adaptive computation time (ACT): let some\\ntokens stop early if they‚Äôre ‚Äúdone.‚Äù This gave a proof-of-\\nconcept for per-token compute.\\n Universal Transformers\\nInstead of stacking dozens of separate layers, Universal\\nTransformers reuse the same layer again and again. It‚Äôs like giving\\ntokens multiple passes through one smart filter.\\nThe architecture improves expressivity and efficiency\\ncompared to vanilla Transformers.\\nIt reduces redundancy by sharing parameters across layers.\\nSource'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nMoE puts many experts in a layer and routes each token to a\\nfew experts. You get huge parameter capacity while keeping\\nper-token compute low when routing is sparse.\\n2. Mixture of Experts\\nExperts - Each FFNN layer now has a set of ‚Äúexperts‚Äù of\\nwhich a subset can be chosen. These ‚Äúexperts‚Äù are\\ntypically FFNNs themselves.\\nRouter or gate network - Determines which tokens are\\nsent to which experts.\\nTwo main components define a MoE:\\nSource'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nMoE answered which specialist to use; recursion asks how\\nmany times to reuse the same block. Both are conditional\\ncompute but with different practical tradeoffs.\\nAt runtime, a router decides which 1 or 2 experts a token\\nshould go to.\\nThat means the model can have billions of parameters in\\ntotal, but each token only activates a tiny fraction of them.\\nYou get the benefit of huge model capacity without paying\\nthe full compute cost on every step.\\nThis made MoE a big deal in scaling LLMs. It showed that\\nyou don‚Äôt always need to throw every parameter at every\\ntoken. Instead, you scale smartly, not just heavily.\\nHow it works?\\nIt showed strong scale efficiency but added routing\\ncomplexity, load balancing issues, and infra overhead (many\\nexperts to manage).\\nThis made MoE a big deal in scaling LLMs. \\nHere is how MoE works:\\nSource'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\n3. Mamba\\nMeet Mamba, a state-space model (a mathematical framework\\nthat tracks and updates an internal state over time to process\\nsequential data efficiently) that‚Äôs making waves as a Transformer\\nalternative.\\nUnlike Transformers, Mamba processes sequences with linear\\ntime complexity, making it blazing fast for long contexts.\\nIt doesn‚Äôt rely on attention. Instead, it uses a selective state\\nspace mechanism to decide what to keep and what to forget.\\nThis means lower memory use, faster inference, and better\\nscaling for massive datasets.\\nWhile Mamba shines in efficiency, it‚Äôs not yet as universally\\ndominant as Transformers in all tasks.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nMoR repeatedly applies a shared transformer block while learned\\nrouters decide per token whether to loop or exit, key-value(KV)\\ncaching is selective for active tokens.\\nThat yields parameter efficiency (fewer unique weights), compute\\nefficiency (tokens stop early), and lower KV overhead (cache only\\nwhat‚Äôs needed).\\nIt‚Äôs trained end-to-end so routing decisions at inference match\\nwhat the model learned.\\n4. Mixture of Recursion'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Predict full recursion depth\\nper token up front and follow\\nthat path. It‚Äôs simpler to\\nbatch but needs a reliable\\ndepth predictor.\\nBhavishya Pandit\\nHow MoR Works?\\nAt each recursion step,\\nrouters pick the top-k tokens\\nthat continue. The active set\\nnarrows with depth; it‚Äôs\\nflexible and dynamic.\\nKV sharing ‚Üí Reuse the first\\npass‚Äôs KV pairs in later\\npasses, slashing prefill\\nlatency and memory costs.\\nStore only the keys/values\\nfor tokens still in play at\\neach step, cutting down\\nmemory reads.\\nE x p e r t - c h o i c e T o k e n - c h o i c e\\nRouting Strategies\\nR e c u r s i o n -\\nw i s e  c a c h i n g\\nK V  s h a r i n g\\nKV Caching Tactics\\nRouting decides how deep each token goes and caching makes sure\\nyou don‚Äôt blow your memory budget doing it. Together, they‚Äôre the\\nreason MoR can scale without rewriting entire decoding stacks.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Model Core Idea\\nAdvancement\\nOver\\nPrevious\\nLimitations\\nVanilla\\nTransformers\\nReplace recurrence\\nwith self-attention +\\nfeed-forward layers,\\nenabling massive\\nparallelism.\\nScales training to\\nbillions of tokens;\\nfoundation of\\nmodern LLMs.\\nUniform compute\\nfor all tokens; large\\nKV caches; high\\ninference cost.\\nUniversal\\nTransformers\\nReuse the same\\nblock recursively so\\ntokens can adapt\\ntheir depth of\\nprocessing.\\nAdaptive depth vs\\nfixed layers in\\nTransformers.\\nSequential per\\nrecursion and slower\\ntraining.\\nMixture of\\nExperts (MoE)\\nRoute each token to\\na small subset of\\nexperts instead of\\nusing all parameters.\\nMassive scaling\\nwithout activating\\nfull model each step.\\nRouting can be\\nunstable; training\\ncomplexity\\nincreases.\\nMamba\\nState-space model\\nreplacing attention\\nwith efficient\\nrecurrence.\\nHandles very long\\nsequences without\\nhuge KV caches.\\nStill new, ecosystem\\nand tooling less\\nmature compared to\\nTransformers.\\nMixture of\\nRecursion\\n(MoR)\\nTokens take different\\nrecursive depths,\\nexiting early or going\\ndeeper as needed.\\nSaves compute on\\neasy tokens; boosts\\naccuracy at smaller\\nscale.\\nRequires careful\\nrecursion control;\\nhardware support\\nstill evolving.\\nBhavishya Pandit\\nLet‚Äôs Have a Comparision'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content=\"Bhavishya Pandit\\nJoin our newsletter for:\\n Step-by-step guides to mastering complex topics\\n Industry trends & innovations delivered straight to your inbox\\n Actionable tips to enhance your skills and stay competitive\\nInsights on cutting-edge AI & software development\\nüí°  Whether you're a developer, researcher, or tech enthusiast, this newsletter is your\\nshortcut to staying informed and ahead of the curve.\\nStay Ahead with Our Tech Newsletter! üöÄ\\nüëâ  Join 1.1k+ leaders and professionals to stay ahead in GenAI!\\nüîó  https://bhavishyapandit9.substack.com/\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nR E P O S T\\nL I K E\\nC O M M E N T\\nFollow to stay updated on\\nGenerative AI'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-05-23T17:27:18+05:30', 'author': 'Aabid M K', 'moddate': '2025-05-23T17:27:18+05:30', 'source': '..\\\\data\\\\pdf\\\\AI AudioBook Generator.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'AI AudioBook Generator.pdf', 'file_type': 'pdf'}, page_content='AudioBook Generator \\n1. Introduction / Objective \\nAudioBook Generator is a web application that allows users to upload one or more text \\ndocuments (PDF, DOCX, TXT) and automatically converts them into high-quality \\naudiobooks. The application leverages Large Language Models (LLMs) to rewrite extracted \\ntext in an engaging, listener-friendly ‚Äúaudiobook style‚Äù before using open-source Text-to-\\nSpeech (TTS) technology to produce downloadable audio files. This project enhances \\naccessibility, productivity, and the enjoyment of written content. \\n \\n2. Methodology / Workflow \\n1. User Uploads Documents \\no Users select and upload one or more documents through an interactive \\nStreamlit web interface. \\n2. Text Extraction \\no The backend parses uploaded files and extracts text content: \\n‚ñ™ PDF: PyPDF2 or pdfplumber \\n‚ñ™ DOCX: python-docx \\n‚ñ™ TXT: Native file reading \\n3. LLM-Based Text Enrichment \\no Extracted text is processed by a Large Language Model (e.g., OpenAI API, \\nGemini API, or open-source LLM) to rewrite the text for better narration and \\nlistener experience. \\no Example LLM prompts: ‚ÄúRewrite this text for an engaging audiobook \\nnarration.‚Äù \\n4. Text-to-Speech Conversion \\no The enriched text is fed into an open-source TTS library (such as pyttsx3, \\nCoqui TTS, or Tortoise TTS), producing a high-quality .mp3 or .wav audio \\nfile. \\n5. Audio Download \\no The generated audio file is presented for immediate download within the \\nStreamlit UI. \\n \\n3. Modules \\n‚Ä¢ Document Upload Module: Handles file uploads via Streamlit. \\n‚Ä¢ Text Extraction Module: Extracts raw text from PDFs, DOCX, and TXT files. \\n‚Ä¢ LLM Enrichment Module: Calls the LLM to rewrite and enhance extracted text. \\n‚Ä¢ Text-to-Speech Module: Converts enriched text into audio using a TTS library. \\n‚Ä¢ Audio Delivery Module: Provides the final audio file to the user for download.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-05-23T17:27:18+05:30', 'author': 'Aabid M K', 'moddate': '2025-05-23T17:27:18+05:30', 'source': '..\\\\data\\\\pdf\\\\AI AudioBook Generator.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'AI AudioBook Generator.pdf', 'file_type': 'pdf'}, page_content='4. Week-wise Module Implementation and High-Level \\nRequirements \\nWeeks 1‚Äì2: \\n‚Ä¢ Set up environment and install dependencies. \\n‚Ä¢ Implement file upload and multi-format text extraction. \\nWeeks 3‚Äì4: \\n‚Ä¢ Integrate LLM for audiobook-style text rewriting. \\n‚Ä¢ Build API connection between Streamlit and backend LLM processing. \\nWeeks 5‚Äì6: \\n‚Ä¢ Integrate and test open-source TTS conversion. \\n‚Ä¢ Ensure support for different voice options and error handling. \\nWeeks 7‚Äì8: \\n‚Ä¢ Finalize UI/UX in Streamlit. \\n‚Ä¢ Conduct thorough testing, optimize performance, and complete documentation. \\n \\n5. Evaluation Criteria \\n‚Ä¢ Milestone 1 (Week 2): \\nFile upload and accurate text extraction operational. \\n‚Ä¢ Milestone 2 (Week 4): \\nLLM-based text rewriting working and demonstrably improving narration. \\n‚Ä¢ Milestone 3 (Week 6): \\nAudio file generation (from rewritten text) stable and high-quality. \\n‚Ä¢ Milestone 4 (Week 8): \\nFull application workflow‚Äîdocument upload to audio download‚Äîoperational, user-\\nfriendly, and documented.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-05-23T17:27:18+05:30', 'author': 'Aabid M K', 'moddate': '2025-05-23T17:27:18+05:30', 'source': '..\\\\data\\\\pdf\\\\AI AudioBook Generator.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3', 'source_file': 'AI AudioBook Generator.pdf', 'file_type': 'pdf'}, page_content='6. Design / Architectural Diagram \\n \\n7. Technology Stack \\n‚Ä¢ Frontend: Streamlit \\n‚Ä¢ Backend: FastAPI or Flask (optional, for modularity or scale) \\n‚Ä¢ Text Extraction: PyPDF2, pdfplumber, python-docx \\n‚Ä¢ LLM Integration: OpenAI API, Gemini API, or local open-source LLM \\n‚Ä¢ Text-to-Speech: pyttsx3, Coqui TTS, Tortoise TTS, or gTTS \\n‚Ä¢ Programming Language: Python 3.x'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='Name DEPARTMENT\\nAnand Chapke Information Technology\\nHardik Rokde Artificial Inteligence &  Data Science \\n \\nVISHWAKARMA INSTITUTE OF TECHNOLOGY, PUNE\\n College Pe \\n‡§ö ‡§ö ‡§æ \\x00 \\nVision to Venture:\\nEMBARK‚Äô25\\nTEAM :'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nP r o b l e m s  :\\nEven after scoring well, there\\nis a mismatch between\\nexpectations and reality.\\n  Cutoffs don‚Äôt reflect academic\\npressure, culture, placement focus,\\nor flexibility for other goals (like\\nstartups or higher studies).\\n80% of students don‚Äôt get to know their\\ncollege until they arrive for admission\\nleads to uninformed decision due to lack\\nof real-time knowledge about the college\\n 60%+ depend on counsellors charging\\n‚Çπ 3,000‚Äì ‚Çπ 20,000  but 80% report\\ndissatisfaction, as suggestions are generic\\nand data-driven, not experience-driven.\\n No reliable platform today\\nprovides honest, branch-specific\\ncollege insights.\\n High Scorers, Average\\nCollege Reality\\nP ast y ear cut of f s ‚â† the full pictur e\\n College Reality Revealed \\nOnly After Admission\\n Expensive Counselling, Poor\\nStudent Satisfaction\\nNo Reliable Source for\\nCollege Truth\\nP R O B L E M\\nBusiness Overview'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nConnect with\\nVerified Seniors\\nfrom your dream\\ncolleges ‚Äì before\\nmaking your final\\ndecision.\\nUnfiltered, Real-\\nTime Insights about\\ncampus life,\\nacademics, and\\nplacements ‚Äì\\nstraight from\\nstudents living it.\\nBranch-Specific\\nGuidance,not just\\ncollege name but\\nstream-specific\\nadvice from those\\nactually pursuing it.\\nTalk Directly to\\nVerified Seniors\\nGet Unfiltered,\\nReal Student\\nInsights\\nBranch-Specific\\nGuidance\\n Free from\\nCounselor Bias ‚Äì\\nno commission-\\nbased guidance\\nor generic college\\nlists.\\nNo Bias, No\\nCommissions Helps students\\nmake goals aligned\\ndecisions,be it for\\njobs, startups, or\\nhigher studies.\\nDecisions\\nAligned with\\nCareer Goals\\nThe Solution ‚Äì College Pe \\n‡§ö ‡§ö ‡§æ \\x00'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nScope of College Pe \\n‡§ö ‡§ö ‡§æ \\x00 \\nCurrently around 8 lac students (engineering & medical combined) registered for  Maharashtra CET which is the\\ncentralized exam for top Maharashtra colleges.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nBuild a reputable personal brand\\nfor future opportunities.\\nPersonal Brand Building\\n03\\nExpand network with aspiring and\\ncurrent students from various colleges.\\nMonetize expertise through\\nstructured, on-demand guidance.\\nNetwork Expansion\\nGuidance to Income\\n01\\n02\\nThe Mentors (College Seniors & Alumni)\\nJEE/MHT-CET focused, interested in\\njobs/startups/higher studies.\\nSupport NEET aspirants with college selection,\\npreparation tips, and branch insights.\\nEngineering Aspirants\\nMedical Aspirants\\n01\\n02\\nAssist CLAT and other domain aspirants with\\nreal-world guidance and career alignment.\\nLaw & Other Domains\\n03\\nThe Seekers (Students & Aspirants)\\nOur Ecosystem- The Seekers & The Mentors\\nBy Role: Mentor vs. Mentee                      By Stream: Engineering | Management | Medical | Others        By Goal: Jobs | Startups | Higher Studies\\xa0|\\xa0Research\\nS e g m e n ta tio n  C rite ria :'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nSubscription-Based\\nModel\\nSeekers: ‚Çπ 499/mo or\\n‚Çπ 3,999/yr\\nMentors: ‚Çπ 199/mo for\\npremium profile\\nUnlimited sessions, AI\\ntools, webinars\\nPay-Per-Session\\nModel\\n‚Çπ 199‚Äì ‚Çπ 499 / 30-\\nmin session\\n70% mentor / 30%\\nplatform split\\nSOURCE 3\\nPremium AI\\nTools\\n‚Çπ 299/mo or ‚Çπ 1,999/yr\\nFit scoring, placement\\nprojections, goal-\\nbased suggestions\\nSOURCE 4\\nCorporate / EdTech\\nTie-Ups\\n‚Çπ 500‚Äì ‚Çπ 1,000 per\\nstudent license\\nBulk mentor\\nsessions + AI tools\\nSOURCE 5\\nWebinars &\\nWorkshops\\nL i v e :  ‚Çπ 9 9 ‚Äì ‚Çπ 2 9 9  p e r\\na t t e n d e e\\nR e c o r d e d  l i b r a r y :\\n‚Çπ 4 9 9 / y e a r\\nRevenue Model \\nSOURCE 2SOURCE 1\\nWe have 5 major revenue sources:'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nUses the latest admission\\ntrends, placement stats, and\\nreal student inputs for highly\\naccurate predictions.\\nA I - P o w e r e d ,  R e c e n t  D a t a -\\nD r i v e n  C o l l e g e  P r e d i c t o r\\nCustomized lists aligned to\\neach student‚Äôs goals, secured\\nto prevent sharing or misuse\\nS e c u r e ,  N o n - F o r w a r d a b l e\\nP e r s o n a l i z e d  C o l l e g e  L i s t\\nD i r e c t  g u i d a n c e  f r o m  v e r i f i e d\\ns e n i o r s  d u r i n g  t h e  c r i t i c a l  c o l l e g e\\np r e f e r e n c e  s e l e c t i o n  p h a s e .\\n1 - o n - 1  E x p e r t  M e n t o r\\nS e s s i o n s\\nS i d e - b y - s i d e  a n a l y s i s  o f\\nc o l l e g e s  b a s e d  o n  c a r e e r  g o a l s ,\\na c a d e m i c s ,  c a m p u s  l i f e ,  a n d\\no p p o r t u n i t i e s .\\nS m a r t  C o l l e g e  C o m p a r i s o n\\nG e n e r a t o r\\nOur USPs :'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\n1 2 K +  W e b s i t e  V i s i t o r s  \\n2 . 5 K +  A c t i v e  U s e r s  \\n1 2 0 +  V e r i f i e d  M e n t o r s\\n‚Äî Students who explored our platform and benefited from our guidance.\\n‚Äî Regularly interacting with seniors, asking questions, and seeking mentorship\\n‚Äî From top engineering colleges across Maharashtra.\\nC o l l e g e s  C o v e r e d :\\nCOEP\\nPICT\\nSPIT\\nCummins\\nWCOES\\nVIT‚ÄôPune\\nA n d  1 5 +  o t h e r  t o p\\nc o l l e g e s  f r o m\\nM a h a r a s h t r a\\nLive Implementation Milestones :\\nF i n d  U s  H e r e\\n@Techz DADA\\n@collegepecharcha\\n@college-pe-charcha\\ncollegepecharcha11@gmail.com\\nwww.techzdada.in'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nCustomer Reviews:'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nTHANK YOU')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95dbce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting into chunks\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    if split_docs:\n",
    "        print(f\"Example chunk: \")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\\n\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\\n\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45217ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 26 documents into 29 chunks\n",
      "Example chunk: \n",
      "Content: T y p e s  o f  L L M\n",
      "A r c h i t e c t u r e s\n",
      "Bhavishya Pandit...\n",
      "\n",
      "Metadata: {'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='T y p e s  o f  L L M\\nA r c h i t e c t u r e s\\nBhavishya Pandit'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nIntroduction\\nTransformers are everywhere. They‚Äôre the groundwork for\\nnearly every major language model today. \\nTransformers\\nMixture of Experts (MoE)\\nMamba\\nMixture of Recursion (MoR)\\nIn this post, we‚Äôll walk through the evolution of four key\\narchitectures that shaped modern AI models:\\nEach step builds on the last, and together they show where the\\nfuture of model design is heading. Let‚Äôs begin with the\\nfoundation of it all, the Transformer.\\nTransformers2017\\nMoE2021\\nMamba2023\\nMoR2025'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nSource\\nInstead of passing information step by step like RNNs or LSTMs,\\ntransfomers introduced self-attention and feed-forward layers so\\ntokens could look at each other all at once. That introduced te\\nconcept of parallelism.\\nTraining on massive datasets without crawling speeds\\nScaling models from millions to billions of parameters\\n1. Transformers\\nThink of Transformers as the moment deep learning hit turbo\\nmode.\\nThis shift unlocked two big things:'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Parallel training means they don‚Äôt have to process words\\none by one like RNNs did. Huge speed boost.\\nThey handle long-range context really well‚Äîperfect for\\nunderstanding long documents or conversations.\\nMost importantly, they scale beautifully. \\nBhavishya Pandit\\nSelf-attention computes relationships between all tokens\\nin a sequence, allowing each token to weigh the\\nimportance of others.\\nFeed-forward layers transform token representations into\\nhigher-level features.\\nStacked layers progressively refine these representations,\\nenabling deep contextual understanding.\\nEvery token, whether it‚Äôs a rare scientific term or a\\ncommon word,goes through the same heavy computation.\\nThis makes inference slow and costly, since memory and\\ncompute pile up.\\n1. Transformers\\nHow do they work:\\nWhy they became so powerful:\\nBut here‚Äôs the catch:\\nContd.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nThey tied weights across depth and applied the same block\\nrepeatedly to refine token states, like an iterative loop. It\\nintroduced the idea of per-position iterative processing.\\nIt also explored adaptive computation time (ACT): let some\\ntokens stop early if they‚Äôre ‚Äúdone.‚Äù This gave a proof-of-\\nconcept for per-token compute.\\n Universal Transformers\\nInstead of stacking dozens of separate layers, Universal\\nTransformers reuse the same layer again and again. It‚Äôs like giving\\ntokens multiple passes through one smart filter.\\nThe architecture improves expressivity and efficiency\\ncompared to vanilla Transformers.\\nIt reduces redundancy by sharing parameters across layers.\\nSource'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nMoE puts many experts in a layer and routes each token to a\\nfew experts. You get huge parameter capacity while keeping\\nper-token compute low when routing is sparse.\\n2. Mixture of Experts\\nExperts - Each FFNN layer now has a set of ‚Äúexperts‚Äù of\\nwhich a subset can be chosen. These ‚Äúexperts‚Äù are\\ntypically FFNNs themselves.\\nRouter or gate network - Determines which tokens are\\nsent to which experts.\\nTwo main components define a MoE:\\nSource'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nMoE answered which specialist to use; recursion asks how\\nmany times to reuse the same block. Both are conditional\\ncompute but with different practical tradeoffs.\\nAt runtime, a router decides which 1 or 2 experts a token\\nshould go to.\\nThat means the model can have billions of parameters in\\ntotal, but each token only activates a tiny fraction of them.\\nYou get the benefit of huge model capacity without paying\\nthe full compute cost on every step.\\nThis made MoE a big deal in scaling LLMs. It showed that\\nyou don‚Äôt always need to throw every parameter at every\\ntoken. Instead, you scale smartly, not just heavily.\\nHow it works?\\nIt showed strong scale efficiency but added routing\\ncomplexity, load balancing issues, and infra overhead (many\\nexperts to manage).\\nThis made MoE a big deal in scaling LLMs. \\nHere is how MoE works:\\nSource'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\n3. Mamba\\nMeet Mamba, a state-space model (a mathematical framework\\nthat tracks and updates an internal state over time to process\\nsequential data efficiently) that‚Äôs making waves as a Transformer\\nalternative.\\nUnlike Transformers, Mamba processes sequences with linear\\ntime complexity, making it blazing fast for long contexts.\\nIt doesn‚Äôt rely on attention. Instead, it uses a selective state\\nspace mechanism to decide what to keep and what to forget.\\nThis means lower memory use, faster inference, and better\\nscaling for massive datasets.\\nWhile Mamba shines in efficiency, it‚Äôs not yet as universally\\ndominant as Transformers in all tasks.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nMoR repeatedly applies a shared transformer block while learned\\nrouters decide per token whether to loop or exit, key-value(KV)\\ncaching is selective for active tokens.\\nThat yields parameter efficiency (fewer unique weights), compute\\nefficiency (tokens stop early), and lower KV overhead (cache only\\nwhat‚Äôs needed).\\nIt‚Äôs trained end-to-end so routing decisions at inference match\\nwhat the model learned.\\n4. Mixture of Recursion'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Predict full recursion depth\\nper token up front and follow\\nthat path. It‚Äôs simpler to\\nbatch but needs a reliable\\ndepth predictor.\\nBhavishya Pandit\\nHow MoR Works?\\nAt each recursion step,\\nrouters pick the top-k tokens\\nthat continue. The active set\\nnarrows with depth; it‚Äôs\\nflexible and dynamic.\\nKV sharing ‚Üí Reuse the first\\npass‚Äôs KV pairs in later\\npasses, slashing prefill\\nlatency and memory costs.\\nStore only the keys/values\\nfor tokens still in play at\\neach step, cutting down\\nmemory reads.\\nE x p e r t - c h o i c e T o k e n - c h o i c e\\nRouting Strategies\\nR e c u r s i o n -\\nw i s e  c a c h i n g\\nK V  s h a r i n g\\nKV Caching Tactics\\nRouting decides how deep each token goes and caching makes sure\\nyou don‚Äôt blow your memory budget doing it. Together, they‚Äôre the\\nreason MoR can scale without rewriting entire decoding stacks.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Model Core Idea\\nAdvancement\\nOver\\nPrevious\\nLimitations\\nVanilla\\nTransformers\\nReplace recurrence\\nwith self-attention +\\nfeed-forward layers,\\nenabling massive\\nparallelism.\\nScales training to\\nbillions of tokens;\\nfoundation of\\nmodern LLMs.\\nUniform compute\\nfor all tokens; large\\nKV caches; high\\ninference cost.\\nUniversal\\nTransformers\\nReuse the same\\nblock recursively so\\ntokens can adapt\\ntheir depth of\\nprocessing.\\nAdaptive depth vs\\nfixed layers in\\nTransformers.\\nSequential per\\nrecursion and slower\\ntraining.\\nMixture of\\nExperts (MoE)\\nRoute each token to\\na small subset of\\nexperts instead of\\nusing all parameters.\\nMassive scaling\\nwithout activating\\nfull model each step.\\nRouting can be\\nunstable; training\\ncomplexity\\nincreases.\\nMamba\\nState-space model\\nreplacing attention\\nwith efficient\\nrecurrence.\\nHandles very long\\nsequences without\\nhuge KV caches.\\nStill new, ecosystem\\nand tooling less\\nmature compared to\\nTransformers.\\nMixture of\\nRecursion\\n(MoR)\\nTokens take different\\nrecursive depths,\\nexiting early or going'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='sequences without\\nhuge KV caches.\\nStill new, ecosystem\\nand tooling less\\nmature compared to\\nTransformers.\\nMixture of\\nRecursion\\n(MoR)\\nTokens take different\\nrecursive depths,\\nexiting early or going\\ndeeper as needed.\\nSaves compute on\\neasy tokens; boosts\\naccuracy at smaller\\nscale.\\nRequires careful\\nrecursion control;\\nhardware support\\nstill evolving.\\nBhavishya Pandit\\nLet‚Äôs Have a Comparision'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content=\"Bhavishya Pandit\\nJoin our newsletter for:\\n Step-by-step guides to mastering complex topics\\n Industry trends & innovations delivered straight to your inbox\\n Actionable tips to enhance your skills and stay competitive\\nInsights on cutting-edge AI & software development\\nüí°  Whether you're a developer, researcher, or tech enthusiast, this newsletter is your\\nshortcut to staying informed and ahead of the curve.\\nStay Ahead with Our Tech Newsletter! üöÄ\\nüëâ  Join 1.1k+ leaders and professionals to stay ahead in GenAI!\\nüîó  https://bhavishyapandit9.substack.com/\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-17T04:52:00+00:00', 'title': 'Types of LLM Archs', 'moddate': '2025-08-17T04:51:57+00:00', 'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0', 'author': 'content team', 'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13', 'source_file': '4 Types of LLM Architectueres.pdf', 'file_type': 'pdf'}, page_content='Bhavishya Pandit\\nR E P O S T\\nL I K E\\nC O M M E N T\\nFollow to stay updated on\\nGenerative AI'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-05-23T17:27:18+05:30', 'author': 'Aabid M K', 'moddate': '2025-05-23T17:27:18+05:30', 'source': '..\\\\data\\\\pdf\\\\AI AudioBook Generator.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'AI AudioBook Generator.pdf', 'file_type': 'pdf'}, page_content='AudioBook Generator \\n1. Introduction / Objective \\nAudioBook Generator is a web application that allows users to upload one or more text \\ndocuments (PDF, DOCX, TXT) and automatically converts them into high-quality \\naudiobooks. The application leverages Large Language Models (LLMs) to rewrite extracted \\ntext in an engaging, listener-friendly ‚Äúaudiobook style‚Äù before using open-source Text-to-\\nSpeech (TTS) technology to produce downloadable audio files. This project enhances \\naccessibility, productivity, and the enjoyment of written content. \\n \\n2. Methodology / Workflow \\n1. User Uploads Documents \\no Users select and upload one or more documents through an interactive \\nStreamlit web interface. \\n2. Text Extraction \\no The backend parses uploaded files and extracts text content: \\n‚ñ™ PDF: PyPDF2 or pdfplumber \\n‚ñ™ DOCX: python-docx \\n‚ñ™ TXT: Native file reading \\n3. LLM-Based Text Enrichment \\no Extracted text is processed by a Large Language Model (e.g., OpenAI API,'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-05-23T17:27:18+05:30', 'author': 'Aabid M K', 'moddate': '2025-05-23T17:27:18+05:30', 'source': '..\\\\data\\\\pdf\\\\AI AudioBook Generator.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'AI AudioBook Generator.pdf', 'file_type': 'pdf'}, page_content='‚ñ™ PDF: PyPDF2 or pdfplumber \\n‚ñ™ DOCX: python-docx \\n‚ñ™ TXT: Native file reading \\n3. LLM-Based Text Enrichment \\no Extracted text is processed by a Large Language Model (e.g., OpenAI API, \\nGemini API, or open-source LLM) to rewrite the text for better narration and \\nlistener experience. \\no Example LLM prompts: ‚ÄúRewrite this text for an engaging audiobook \\nnarration.‚Äù \\n4. Text-to-Speech Conversion \\no The enriched text is fed into an open-source TTS library (such as pyttsx3, \\nCoqui TTS, or Tortoise TTS), producing a high-quality .mp3 or .wav audio \\nfile. \\n5. Audio Download \\no The generated audio file is presented for immediate download within the \\nStreamlit UI. \\n \\n3. Modules \\n‚Ä¢ Document Upload Module: Handles file uploads via Streamlit. \\n‚Ä¢ Text Extraction Module: Extracts raw text from PDFs, DOCX, and TXT files. \\n‚Ä¢ LLM Enrichment Module: Calls the LLM to rewrite and enhance extracted text. \\n‚Ä¢ Text-to-Speech Module: Converts enriched text into audio using a TTS library.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-05-23T17:27:18+05:30', 'author': 'Aabid M K', 'moddate': '2025-05-23T17:27:18+05:30', 'source': '..\\\\data\\\\pdf\\\\AI AudioBook Generator.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'AI AudioBook Generator.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ LLM Enrichment Module: Calls the LLM to rewrite and enhance extracted text. \\n‚Ä¢ Text-to-Speech Module: Converts enriched text into audio using a TTS library. \\n‚Ä¢ Audio Delivery Module: Provides the final audio file to the user for download.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-05-23T17:27:18+05:30', 'author': 'Aabid M K', 'moddate': '2025-05-23T17:27:18+05:30', 'source': '..\\\\data\\\\pdf\\\\AI AudioBook Generator.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'AI AudioBook Generator.pdf', 'file_type': 'pdf'}, page_content='4. Week-wise Module Implementation and High-Level \\nRequirements \\nWeeks 1‚Äì2: \\n‚Ä¢ Set up environment and install dependencies. \\n‚Ä¢ Implement file upload and multi-format text extraction. \\nWeeks 3‚Äì4: \\n‚Ä¢ Integrate LLM for audiobook-style text rewriting. \\n‚Ä¢ Build API connection between Streamlit and backend LLM processing. \\nWeeks 5‚Äì6: \\n‚Ä¢ Integrate and test open-source TTS conversion. \\n‚Ä¢ Ensure support for different voice options and error handling. \\nWeeks 7‚Äì8: \\n‚Ä¢ Finalize UI/UX in Streamlit. \\n‚Ä¢ Conduct thorough testing, optimize performance, and complete documentation. \\n \\n5. Evaluation Criteria \\n‚Ä¢ Milestone 1 (Week 2): \\nFile upload and accurate text extraction operational. \\n‚Ä¢ Milestone 2 (Week 4): \\nLLM-based text rewriting working and demonstrably improving narration. \\n‚Ä¢ Milestone 3 (Week 6): \\nAudio file generation (from rewritten text) stable and high-quality. \\n‚Ä¢ Milestone 4 (Week 8): \\nFull application workflow‚Äîdocument upload to audio download‚Äîoperational, user-\\nfriendly, and documented.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-05-23T17:27:18+05:30', 'author': 'Aabid M K', 'moddate': '2025-05-23T17:27:18+05:30', 'source': '..\\\\data\\\\pdf\\\\AI AudioBook Generator.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3', 'source_file': 'AI AudioBook Generator.pdf', 'file_type': 'pdf'}, page_content='6. Design / Architectural Diagram \\n \\n7. Technology Stack \\n‚Ä¢ Frontend: Streamlit \\n‚Ä¢ Backend: FastAPI or Flask (optional, for modularity or scale) \\n‚Ä¢ Text Extraction: PyPDF2, pdfplumber, python-docx \\n‚Ä¢ LLM Integration: OpenAI API, Gemini API, or local open-source LLM \\n‚Ä¢ Text-to-Speech: pyttsx3, Coqui TTS, Tortoise TTS, or gTTS \\n‚Ä¢ Programming Language: Python 3.x'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='Name DEPARTMENT\\nAnand Chapke Information Technology\\nHardik Rokde Artificial Inteligence &  Data Science \\n \\nVISHWAKARMA INSTITUTE OF TECHNOLOGY, PUNE\\n College Pe \\n‡§ö ‡§ö ‡§æ \\x00 \\nVision to Venture:\\nEMBARK‚Äô25\\nTEAM :'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nP r o b l e m s  :\\nEven after scoring well, there\\nis a mismatch between\\nexpectations and reality.\\n  Cutoffs don‚Äôt reflect academic\\npressure, culture, placement focus,\\nor flexibility for other goals (like\\nstartups or higher studies).\\n80% of students don‚Äôt get to know their\\ncollege until they arrive for admission\\nleads to uninformed decision due to lack\\nof real-time knowledge about the college\\n 60%+ depend on counsellors charging\\n‚Çπ 3,000‚Äì ‚Çπ 20,000  but 80% report\\ndissatisfaction, as suggestions are generic\\nand data-driven, not experience-driven.\\n No reliable platform today\\nprovides honest, branch-specific\\ncollege insights.\\n High Scorers, Average\\nCollege Reality\\nP ast y ear cut of f s ‚â† the full pictur e\\n College Reality Revealed \\nOnly After Admission\\n Expensive Counselling, Poor\\nStudent Satisfaction\\nNo Reliable Source for\\nCollege Truth\\nP R O B L E M\\nBusiness Overview'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nConnect with\\nVerified Seniors\\nfrom your dream\\ncolleges ‚Äì before\\nmaking your final\\ndecision.\\nUnfiltered, Real-\\nTime Insights about\\ncampus life,\\nacademics, and\\nplacements ‚Äì\\nstraight from\\nstudents living it.\\nBranch-Specific\\nGuidance,not just\\ncollege name but\\nstream-specific\\nadvice from those\\nactually pursuing it.\\nTalk Directly to\\nVerified Seniors\\nGet Unfiltered,\\nReal Student\\nInsights\\nBranch-Specific\\nGuidance\\n Free from\\nCounselor Bias ‚Äì\\nno commission-\\nbased guidance\\nor generic college\\nlists.\\nNo Bias, No\\nCommissions Helps students\\nmake goals aligned\\ndecisions,be it for\\njobs, startups, or\\nhigher studies.\\nDecisions\\nAligned with\\nCareer Goals\\nThe Solution ‚Äì College Pe \\n‡§ö ‡§ö ‡§æ \\x00'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nScope of College Pe \\n‡§ö ‡§ö ‡§æ \\x00 \\nCurrently around 8 lac students (engineering & medical combined) registered for  Maharashtra CET which is the\\ncentralized exam for top Maharashtra colleges.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nBuild a reputable personal brand\\nfor future opportunities.\\nPersonal Brand Building\\n03\\nExpand network with aspiring and\\ncurrent students from various colleges.\\nMonetize expertise through\\nstructured, on-demand guidance.\\nNetwork Expansion\\nGuidance to Income\\n01\\n02\\nThe Mentors (College Seniors & Alumni)\\nJEE/MHT-CET focused, interested in\\njobs/startups/higher studies.\\nSupport NEET aspirants with college selection,\\npreparation tips, and branch insights.\\nEngineering Aspirants\\nMedical Aspirants\\n01\\n02\\nAssist CLAT and other domain aspirants with\\nreal-world guidance and career alignment.\\nLaw & Other Domains\\n03\\nThe Seekers (Students & Aspirants)\\nOur Ecosystem- The Seekers & The Mentors\\nBy Role: Mentor vs. Mentee                      By Stream: Engineering | Management | Medical | Others        By Goal: Jobs | Startups | Higher Studies\\xa0|\\xa0Research\\nS e g m e n ta tio n  C rite ria :'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nSubscription-Based\\nModel\\nSeekers: ‚Çπ 499/mo or\\n‚Çπ 3,999/yr\\nMentors: ‚Çπ 199/mo for\\npremium profile\\nUnlimited sessions, AI\\ntools, webinars\\nPay-Per-Session\\nModel\\n‚Çπ 199‚Äì ‚Çπ 499 / 30-\\nmin session\\n70% mentor / 30%\\nplatform split\\nSOURCE 3\\nPremium AI\\nTools\\n‚Çπ 299/mo or ‚Çπ 1,999/yr\\nFit scoring, placement\\nprojections, goal-\\nbased suggestions\\nSOURCE 4\\nCorporate / EdTech\\nTie-Ups\\n‚Çπ 500‚Äì ‚Çπ 1,000 per\\nstudent license\\nBulk mentor\\nsessions + AI tools\\nSOURCE 5\\nWebinars &\\nWorkshops\\nL i v e :  ‚Çπ 9 9 ‚Äì ‚Çπ 2 9 9  p e r\\na t t e n d e e\\nR e c o r d e d  l i b r a r y :\\n‚Çπ 4 9 9 / y e a r\\nRevenue Model \\nSOURCE 2SOURCE 1\\nWe have 5 major revenue sources:'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nUses the latest admission\\ntrends, placement stats, and\\nreal student inputs for highly\\naccurate predictions.\\nA I - P o w e r e d ,  R e c e n t  D a t a -\\nD r i v e n  C o l l e g e  P r e d i c t o r\\nCustomized lists aligned to\\neach student‚Äôs goals, secured\\nto prevent sharing or misuse\\nS e c u r e ,  N o n - F o r w a r d a b l e\\nP e r s o n a l i z e d  C o l l e g e  L i s t\\nD i r e c t  g u i d a n c e  f r o m  v e r i f i e d\\ns e n i o r s  d u r i n g  t h e  c r i t i c a l  c o l l e g e\\np r e f e r e n c e  s e l e c t i o n  p h a s e .\\n1 - o n - 1  E x p e r t  M e n t o r\\nS e s s i o n s\\nS i d e - b y - s i d e  a n a l y s i s  o f\\nc o l l e g e s  b a s e d  o n  c a r e e r  g o a l s ,\\na c a d e m i c s ,  c a m p u s  l i f e ,  a n d\\no p p o r t u n i t i e s .\\nS m a r t  C o l l e g e  C o m p a r i s o n\\nG e n e r a t o r\\nOur USPs :'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\n1 2 K +  W e b s i t e  V i s i t o r s  \\n2 . 5 K +  A c t i v e  U s e r s  \\n1 2 0 +  V e r i f i e d  M e n t o r s\\n‚Äî Students who explored our platform and benefited from our guidance.\\n‚Äî Regularly interacting with seniors, asking questions, and seeking mentorship\\n‚Äî From top engineering colleges across Maharashtra.\\nC o l l e g e s  C o v e r e d :\\nCOEP\\nPICT\\nSPIT\\nCummins\\nWCOES\\nVIT‚ÄôPune\\nA n d  1 5 +  o t h e r  t o p\\nc o l l e g e s  f r o m\\nM a h a r a s h t r a\\nLive Implementation Milestones :\\nF i n d  U s  H e r e\\n@Techz DADA\\n@collegepecharcha\\n@college-pe-charcha\\ncollegepecharcha11@gmail.com\\nwww.techzdada.in'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nCustomer Reviews:'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-12T18:14:35+00:00', 'title': 'CollegePeCharcha_VITPune', 'moddate': '2025-08-12T18:14:30+00:00', 'keywords': 'DAGvge8AbKM,BAFYBP66Xu0,0', 'author': 'Anand', 'source': '..\\\\data\\\\pdf\\\\CollegePeCharcha_VITPune.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10', 'source_file': 'CollegePeCharcha_VITPune.pdf', 'file_type': 'pdf'}, page_content='COLLEGE PE\\n‡§ö‡§ö\\x00\\nTHANK YOU')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7415f3e",
   "metadata": {},
   "source": [
    "### embeddings and vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11ab9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83b52325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x204b6dfb890>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c241957",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20d1a4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x204b72e5a00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f821dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 29 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (29, 384)\n",
      "Adding 29 documents to vector store...\n",
      "Successfully added 29 documents to vector store\n",
      "Total documents in collection: 29\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "# Generate  embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "#store in the vector database\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc835242",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1befea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever = RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4339c17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'How do transformers work?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_f51cd3aa_2',\n",
       "  'content': 'Bhavishya Pandit\\nSource\\nInstead of passing information step by step like RNNs or LSTMs,\\ntransfomers introduced self-attention and feed-forward layers so\\ntokens could look at each other all at once. That introduced te\\nconcept of parallelism.\\nTraining on massive datasets without crawling speeds\\nScaling models from millions to billions of parameters\\n1. Transformers\\nThink of Transformers as the moment deep learning hit turbo\\nmode.\\nThis shift unlocked two big things:',\n",
       "  'metadata': {'content_length': 466,\n",
       "   'title': 'Types of LLM Archs',\n",
       "   'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0',\n",
       "   'source_file': '4 Types of LLM Architectueres.pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf',\n",
       "   'total_pages': 13,\n",
       "   'creationdate': '2025-08-17T04:52:00+00:00',\n",
       "   'author': 'content team',\n",
       "   'page_label': '3',\n",
       "   'file_type': 'pdf',\n",
       "   'page': 2,\n",
       "   'producer': 'Canva',\n",
       "   'creator': 'Canva',\n",
       "   'doc_index': 2,\n",
       "   'moddate': '2025-08-17T04:51:57+00:00'},\n",
       "  'similarity_score': 0.06134837865829468,\n",
       "  'distance': 0.9386516213417053,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_43b3d938_1',\n",
       "  'content': 'Bhavishya Pandit\\nIntroduction\\nTransformers are everywhere. They‚Äôre the groundwork for\\nnearly every major language model today. \\nTransformers\\nMixture of Experts (MoE)\\nMamba\\nMixture of Recursion (MoR)\\nIn this post, we‚Äôll walk through the evolution of four key\\narchitectures that shaped modern AI models:\\nEach step builds on the last, and together they show where the\\nfuture of model design is heading. Let‚Äôs begin with the\\nfoundation of it all, the Transformer.\\nTransformers2017\\nMoE2021\\nMamba2023\\nMoR2025',\n",
       "  'metadata': {'creationdate': '2025-08-17T04:52:00+00:00',\n",
       "   'total_pages': 13,\n",
       "   'source': '..\\\\data\\\\pdf\\\\4 Types of LLM Architectueres.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'author': 'content team',\n",
       "   'title': 'Types of LLM Archs',\n",
       "   'moddate': '2025-08-17T04:51:57+00:00',\n",
       "   'keywords': 'DAGwQeT0M8w,BAGQ0G2eswY,0',\n",
       "   'creator': 'Canva',\n",
       "   'page': 1,\n",
       "   'page_label': '2',\n",
       "   'doc_index': 1,\n",
       "   'producer': 'Canva',\n",
       "   'content_length': 502,\n",
       "   'source_file': '4 Types of LLM Architectueres.pdf'},\n",
       "  'similarity_score': 0.05061531066894531,\n",
       "  'distance': 0.9493846893310547,\n",
       "  'rank': 2}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"How do transformers work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901c026",
   "metadata": {},
   "source": [
    "### Integration of VectorDB Context Pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f272e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Simple RAG Pipeline integrating VectorDB context with LLM output\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize Gemini Chat LLM\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    google_api_key=gemini_api_key,\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query, retriever, llm, top_k = 3):\n",
    "    ## retrieve context\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"No relevant context found.\"\n",
    "\n",
    "    ## generate response\n",
    "    prompt = f\"\"\"Use the following context to answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "\n",
    "    response = llm.invoke([prompt.format(context = context, query = query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30d7b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Explain the transformer architecture.'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformer architecture, introduced in 2017, is considered the foundation of modern AI models and the groundwork for nearly every major language model today. Unlike RNNs or LSTMs that pass information step by step, transformers introduced self-attention and feed-forward layers. This allows tokens to look at each other all at once, which introduced the concept of parallelism. This shift unlocked the ability to train on massive datasets without crawling speeds and scale models from millions to billions of parameters.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"Explain the transformer architecture.\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782db517",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4bc1fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is Audiobook Generator?'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 19.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: AudioBook Generator is a web application that allows users to upload text documents (PDF, DOCX, TXT) and automatically converts them into high-quality audiobooks.\n",
      "Sources: [{'source': 'AI AudioBook Generator.pdf', 'page': 0, 'score': 0.3885383605957031, 'preview': 'AudioBook Generator \\n1. Introduction / Objective \\nAudioBook Generator is a web application that allows users to upload one or more text \\ndocuments (PDF, DOCX, TXT) and automatically converts them into high-quality \\naudiobooks. The application leverages Large Language Models (LLMs) to rewrite extract...'}]\n",
      "Confidence: 0.3885383605957031\n",
      "Context Preview: AudioBook Generator \n",
      "1. Introduction / Objective \n",
      "AudioBook Generator is a web application that allows users to upload one or more text \n",
      "documents (PDF, DOCX, TXT) and automatically converts them into high-quality \n",
      "audiobooks. The application leverages Large Language Models (LLMs) to rewrite extract\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"What is Audiobook Generator?\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07b9411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Explain college reality revealed only after admission'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "COLLEGE PE\n",
      "‡§ö‡§ö\u0000\n",
      "P r o b l e m s  :\n",
      "Even after scoring well, there\n",
      "is a mismatch between\n",
      "expe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctations and reality.\n",
      "  Cutoffs don‚Äôt reflect academic\n",
      "pressure, culture, placement focus,\n",
      "or flexibility for other goals (like\n",
      "startups or higher studies).\n",
      "80% of students don‚Äôt get to know their\n",
      "college until they arrive for admission\n",
      "leads to uninformed decision due to lack\n",
      "of real-time knowledge about the college\n",
      " 60%+ depend on counsellors charging\n",
      "‚Çπ 3,000‚Äì ‚Çπ 20,000  but 80% report\n",
      "dissatisfaction, as suggestions are generic\n",
      "and data-driven, not experience-driven.\n",
      " No reliable platform today\n",
      "provides honest, branch-specific\n",
      "college insights.\n",
      " High Scorers, Average\n",
      "College Reality\n",
      "P ast y ear cut of f s ‚â† the full pictur e\n",
      " College Reality Revealed \n",
      "Only After Admission\n",
      " Expensive Counselling, Poor\n",
      "Student Satisfaction\n",
      "No Reliable Source for\n",
      "College Truth\n",
      "P R O B L E M\n",
      "Business Overview\n",
      "\n",
      "Question: Explain college reality revealed only after admission\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: 80% of students don't get to know their college until they arrive for admission, leading to uninformed decisions due to a lack of real-time knowledge.\n",
      "\n",
      "Citations:\n",
      "[1] CollegePeCharcha_VITPune.pdf (page 1)\n",
      "Summary: A significant majority of students (80%) make college decisions without real-time knowledge, only getting to know their institution upon arrival for admission. This lack of prior information often leads to uninformed choices.\n",
      "History: {'question': 'Explain college reality revealed only after admission', 'answer': \"80% of students don't get to know their college until they arrive for admission, leading to uninformed decisions due to a lack of real-time knowledge.\", 'sources': [{'source': 'CollegePeCharcha_VITPune.pdf', 'page': 1, 'score': 0.1256381869316101, 'preview': 'COLLEGE PE\\n‡§ö‡§ö\\x00\\nP r o b l e m s  :\\nEven after scoring well, there\\nis a mismatch between\\nexpectations and reality.\\n  Cutof...'}], 'summary': 'A significant majority of students (80%) make college decisions without real-time knowledge, only getting to know their institution upon arrival for admission. This lack of prior information often leads to uninformed choices.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer, \n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"Explain college reality revealed only after admission\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bae82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
